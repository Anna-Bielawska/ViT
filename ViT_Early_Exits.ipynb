{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "2be7e01111e946529d401302b1b98e71": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_5b05b1f3aad444ad9d0f51626b0152df",
              "IPY_MODEL_db5ee99d20ee470cbecde88f070da07b",
              "IPY_MODEL_0663381d1ecf49e9963fc95630edd51e"
            ],
            "layout": "IPY_MODEL_4be6f0ec679e457082787eb7f74a59c3"
          }
        },
        "5b05b1f3aad444ad9d0f51626b0152df": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c96ca1db4f624996a86041f41631e0d1",
            "placeholder": "​",
            "style": "IPY_MODEL_c5d964514c6a4382a53fbdd2b5053c42",
            "value": "100%"
          }
        },
        "db5ee99d20ee470cbecde88f070da07b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_084a8a334dc3487382f51e90e44cea14",
            "max": 346328529,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_b91a4bb0b74b4982bcd995cc34f7262a",
            "value": 346328529
          }
        },
        "0663381d1ecf49e9963fc95630edd51e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c469ad4ab88c40b5ba072208884fcd4e",
            "placeholder": "​",
            "style": "IPY_MODEL_81d4a8723a0b45e7b87f134367d33c6b",
            "value": " 330M/330M [00:01&lt;00:00, 229MB/s]"
          }
        },
        "4be6f0ec679e457082787eb7f74a59c3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c96ca1db4f624996a86041f41631e0d1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c5d964514c6a4382a53fbdd2b5053c42": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "084a8a334dc3487382f51e90e44cea14": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b91a4bb0b74b4982bcd995cc34f7262a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "c469ad4ab88c40b5ba072208884fcd4e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "81d4a8723a0b45e7b87f134367d33c6b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "https://github.com/pytorch/vision/blob/main/torchvision/models/vision_transformer.py"
      ],
      "metadata": {
        "id": "NnDk365XvYEr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "from collections import OrderedDict\n",
        "from functools import partial\n",
        "from typing import Any, Callable, Dict, List, NamedTuple, Optional\n",
        "from torchvision.models.vision_transformer import ConvStemConfig, WeightsEnum, MLPBlock, EncoderBlock #, Encoder\n",
        "from torchvision.models import ViT_B_16_Weights  # , vit_b_16\n",
        "from torchvision.utils import _log_api_usage_once\n",
        "from torchvision.models._utils import _ovewrite_named_param\n",
        "from torchvision.ops.misc import Conv2dNormActivation\n",
        "from torchvision.datasets import Flowers102\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "from torchvision.transforms import ToTensor, Compose, Lambda, Resize\n",
        "from sklearn.model_selection import ParameterGrid\n",
        "from collections.abc import Iterable\n",
        "from sklearn.utils import shuffle"
      ],
      "metadata": {
        "id": "BhICkRZF5vCP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder(nn.Module):\n",
        "    \"\"\"Transformer Model Encoder for sequence to sequence translation.\"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        seq_length: int,\n",
        "        num_layers: int,\n",
        "        num_heads: int,\n",
        "        hidden_dim: int,\n",
        "        mlp_dim: int,\n",
        "        dropout: float,\n",
        "        attention_dropout: float,\n",
        "        norm_layer: Callable[..., torch.nn.Module] = partial(nn.LayerNorm, eps=1e-6),\n",
        "    ):\n",
        "        super().__init__()\n",
        "        # Note that batch_size is on the first dim because\n",
        "        # we have batch_first=True in nn.MultiAttention() by default\n",
        "        self.pos_embedding = nn.Parameter(torch.empty(1, seq_length, hidden_dim).normal_(std=0.02))  # from BERT\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        layers: OrderedDict[str, nn.Module] = OrderedDict()\n",
        "        class_heads: OrderedDict[str, nn.Module] = OrderedDict()\n",
        "        for i in range(num_layers):\n",
        "            layers[f\"encoder_layer_{i}\"] = EncoderBlock(\n",
        "                num_heads,\n",
        "                hidden_dim,\n",
        "                mlp_dim,\n",
        "                dropout,\n",
        "                attention_dropout,\n",
        "                norm_layer,\n",
        "            )\n",
        "            class_heads[f\"class_head_{i}\"] = nn.Sequential(\n",
        "                # add some activation?\n",
        "                nn.Linear(in_features=hidden_dim, out_features=102, bias=True),\n",
        "                nn.Softmax(dim=1)\n",
        "            )\n",
        "\n",
        "        self.layers = nn.Sequential(layers)\n",
        "        self.ln = norm_layer(hidden_dim)\n",
        "        self.class_heads = nn.Sequential(class_heads)\n",
        "\n",
        "    def forward(self, input: torch.Tensor):\n",
        "        torch._assert(input.dim() == 3, f\"Expected (batch_size, seq_length, hidden_dim) got {input.shape}\")\n",
        "        _input = input + self.pos_embedding\n",
        "        _input = self.dropout(_input)\n",
        "\n",
        "        early_classification = []\n",
        "        for layer, class_head in zip(self.layers, self.class_heads):\n",
        "            _input = layer(_input)\n",
        "            early_classification.append(class_head(_input[:, 0]))\n",
        "\n",
        "        # return self.ln(self.layers(self.dropout(input)))\n",
        "        return self.ln(_input), early_classification"
      ],
      "metadata": {
        "id": "3USL6bis9qsx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9rlLPq_25ak4"
      },
      "outputs": [],
      "source": [
        "class VisionTransformer(nn.Module):\n",
        "    \"\"\"Vision Transformer as per https://arxiv.org/abs/2010.11929.\"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        image_size: int,\n",
        "        patch_size: int,\n",
        "        num_layers: int,\n",
        "        num_heads: int,\n",
        "        hidden_dim: int,\n",
        "        mlp_dim: int,\n",
        "        dropout: float = 0.0,\n",
        "        attention_dropout: float = 0.0,\n",
        "        num_classes: int = 1000,\n",
        "        representation_size: Optional[int] = None,\n",
        "        norm_layer: Callable[..., torch.nn.Module] = partial(nn.LayerNorm, eps=1e-6),\n",
        "        conv_stem_configs: Optional[List[ConvStemConfig]] = None,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        _log_api_usage_once(self)\n",
        "        torch._assert(image_size % patch_size == 0, \"Input shape indivisible by patch size!\")\n",
        "        self.image_size = image_size\n",
        "        self.patch_size = patch_size\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.mlp_dim = mlp_dim\n",
        "        self.attention_dropout = attention_dropout\n",
        "        self.dropout = dropout\n",
        "        self.num_classes = num_classes\n",
        "        self.representation_size = representation_size\n",
        "        self.norm_layer = norm_layer\n",
        "\n",
        "        if conv_stem_configs is not None:\n",
        "            # As per https://arxiv.org/abs/2106.14881\n",
        "            seq_proj = nn.Sequential()\n",
        "            prev_channels = 3\n",
        "            for i, conv_stem_layer_config in enumerate(conv_stem_configs):\n",
        "                seq_proj.add_module(\n",
        "                    f\"conv_bn_relu_{i}\",\n",
        "                    Conv2dNormActivation(\n",
        "                        in_channels=prev_channels,\n",
        "                        out_channels=conv_stem_layer_config.out_channels,\n",
        "                        kernel_size=conv_stem_layer_config.kernel_size,\n",
        "                        stride=conv_stem_layer_config.stride,\n",
        "                        norm_layer=conv_stem_layer_config.norm_layer,\n",
        "                        activation_layer=conv_stem_layer_config.activation_layer,\n",
        "                    ),\n",
        "                )\n",
        "                prev_channels = conv_stem_layer_config.out_channels\n",
        "            seq_proj.add_module(\n",
        "                \"conv_last\", nn.Conv2d(in_channels=prev_channels, out_channels=hidden_dim, kernel_size=1)\n",
        "            )\n",
        "            self.conv_proj: nn.Module = seq_proj\n",
        "        else:\n",
        "            self.conv_proj = nn.Conv2d(\n",
        "                in_channels=3, out_channels=hidden_dim, kernel_size=patch_size, stride=patch_size\n",
        "            )\n",
        "\n",
        "        seq_length = (image_size // patch_size) ** 2\n",
        "\n",
        "        # Add a class token\n",
        "        self.class_token = nn.Parameter(torch.zeros(1, 1, hidden_dim))\n",
        "        seq_length += 1\n",
        "\n",
        "        self.encoder = Encoder(\n",
        "            seq_length,\n",
        "            num_layers,\n",
        "            num_heads,\n",
        "            hidden_dim,\n",
        "            mlp_dim,\n",
        "            dropout,\n",
        "            attention_dropout,\n",
        "            norm_layer,\n",
        "        )\n",
        "        self.seq_length = seq_length\n",
        "\n",
        "        heads_layers: OrderedDict[str, nn.Module] = OrderedDict()\n",
        "        if representation_size is None:\n",
        "            heads_layers[\"head\"] = nn.Linear(hidden_dim, num_classes)\n",
        "        else:\n",
        "            heads_layers[\"pre_logits\"] = nn.Linear(hidden_dim, representation_size)\n",
        "            heads_layers[\"act\"] = nn.Tanh()\n",
        "            heads_layers[\"head\"] = nn.Linear(representation_size, num_classes)\n",
        "\n",
        "        self.heads = nn.Sequential(heads_layers)\n",
        "\n",
        "        if isinstance(self.conv_proj, nn.Conv2d):\n",
        "            # Init the patchify stem\n",
        "            fan_in = self.conv_proj.in_channels * self.conv_proj.kernel_size[0] * self.conv_proj.kernel_size[1]\n",
        "            nn.init.trunc_normal_(self.conv_proj.weight, std=math.sqrt(1 / fan_in))\n",
        "            if self.conv_proj.bias is not None:\n",
        "                nn.init.zeros_(self.conv_proj.bias)\n",
        "        elif self.conv_proj.conv_last is not None and isinstance(self.conv_proj.conv_last, nn.Conv2d):\n",
        "            # Init the last 1x1 conv of the conv stem\n",
        "            nn.init.normal_(\n",
        "                self.conv_proj.conv_last.weight, mean=0.0, std=math.sqrt(2.0 / self.conv_proj.conv_last.out_channels)\n",
        "            )\n",
        "            if self.conv_proj.conv_last.bias is not None:\n",
        "                nn.init.zeros_(self.conv_proj.conv_last.bias)\n",
        "\n",
        "        if hasattr(self.heads, \"pre_logits\") and isinstance(self.heads.pre_logits, nn.Linear):\n",
        "            fan_in = self.heads.pre_logits.in_features\n",
        "            nn.init.trunc_normal_(self.heads.pre_logits.weight, std=math.sqrt(1 / fan_in))\n",
        "            nn.init.zeros_(self.heads.pre_logits.bias)\n",
        "\n",
        "        if isinstance(self.heads.head, nn.Linear):\n",
        "            nn.init.zeros_(self.heads.head.weight)\n",
        "            nn.init.zeros_(self.heads.head.bias)\n",
        "\n",
        "    def _process_input(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        n, c, h, w = x.shape\n",
        "        p = self.patch_size\n",
        "        torch._assert(h == self.image_size, f\"Wrong image height! Expected {self.image_size} but got {h}!\")\n",
        "        torch._assert(w == self.image_size, f\"Wrong image width! Expected {self.image_size} but got {w}!\")\n",
        "        n_h = h // p\n",
        "        n_w = w // p\n",
        "\n",
        "        # (n, c, h, w) -> (n, hidden_dim, n_h, n_w)\n",
        "        x = self.conv_proj(x)\n",
        "        # (n, hidden_dim, n_h, n_w) -> (n, hidden_dim, (n_h * n_w))\n",
        "        x = x.reshape(n, self.hidden_dim, n_h * n_w)\n",
        "\n",
        "        # (n, hidden_dim, (n_h * n_w)) -> (n, (n_h * n_w), hidden_dim)\n",
        "        # The self attention layer expects inputs in the format (N, S, E)\n",
        "        # where S is the source sequence length, N is the batch size, E is the\n",
        "        # embedding dimension\n",
        "        x = x.permute(0, 2, 1)\n",
        "\n",
        "        return x\n",
        "\n",
        "    def forward(self, x: torch.Tensor):\n",
        "        # Reshape and permute the input tensor\n",
        "        x = self._process_input(x)\n",
        "        n = x.shape[0]\n",
        "\n",
        "        # Expand the class token to the full batch\n",
        "        batch_class_token = self.class_token.expand(n, -1, -1)\n",
        "        x = torch.cat([batch_class_token, x], dim=1)\n",
        "\n",
        "        x, early_classif = self.encoder(x)\n",
        "\n",
        "        # Classifier \"token\" as used by standard language architectures\n",
        "        x = x[:, 0]\n",
        "\n",
        "        x = self.heads(x)\n",
        "\n",
        "        return x, early_classif\n",
        "        \n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def _vision_transformer(\n",
        "    patch_size: int,\n",
        "    num_layers: int,\n",
        "    num_heads: int,\n",
        "    hidden_dim: int,\n",
        "    mlp_dim: int,\n",
        "    weights: Optional[WeightsEnum],\n",
        "    progress: bool,\n",
        "    **kwargs: Any,\n",
        ") -> VisionTransformer:\n",
        "    if weights is not None:\n",
        "        _ovewrite_named_param(kwargs, \"num_classes\", len(weights.meta[\"categories\"]))\n",
        "        assert weights.meta[\"min_size\"][0] == weights.meta[\"min_size\"][1]\n",
        "        _ovewrite_named_param(kwargs, \"image_size\", weights.meta[\"min_size\"][0])\n",
        "    image_size = kwargs.pop(\"image_size\", 224)\n",
        "\n",
        "    model = VisionTransformer(\n",
        "        image_size=image_size,\n",
        "        patch_size=patch_size,\n",
        "        num_layers=num_layers,\n",
        "        num_heads=num_heads,\n",
        "        hidden_dim=hidden_dim,\n",
        "        mlp_dim=mlp_dim,\n",
        "        **kwargs,\n",
        "    )\n",
        "\n",
        "    if weights:\n",
        "        model.load_state_dict(weights.get_state_dict(progress=progress), strict=False)\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "rcNlCdTz8gwN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @register_model()\n",
        "# @handle_legacy_interface(weights=(\"pretrained\", ViT_B_16_Weights.IMAGENET1K_V1))\n",
        "def vit_b_16(*, weights: Optional[ViT_B_16_Weights] = None, progress: bool = True, **kwargs: Any) -> VisionTransformer:\n",
        "    \"\"\"\n",
        "    Constructs a vit_b_16 architecture from\n",
        "    `An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale <https://arxiv.org/abs/2010.11929>`_.\n",
        "    Args:\n",
        "        weights (:class:`~torchvision.models.ViT_B_16_Weights`, optional): The pretrained\n",
        "            weights to use. See :class:`~torchvision.models.ViT_B_16_Weights`\n",
        "            below for more details and possible values. By default, no pre-trained weights are used.\n",
        "        progress (bool, optional): If True, displays a progress bar of the download to stderr. Default is True.\n",
        "        **kwargs: parameters passed to the ``torchvision.models.vision_transformer.VisionTransformer``\n",
        "            base class. Please refer to the `source code\n",
        "            <https://github.com/pytorch/vision/blob/main/torchvision/models/vision_transformer.py>`_\n",
        "            for more details about this class.\n",
        "    .. autoclass:: torchvision.models.ViT_B_16_Weights\n",
        "        :members:\n",
        "    \"\"\"\n",
        "    weights = ViT_B_16_Weights.verify(weights)\n",
        "\n",
        "    return _vision_transformer(\n",
        "        patch_size=16,\n",
        "        num_layers=12,\n",
        "        num_heads=12,\n",
        "        hidden_dim=768,\n",
        "        mlp_dim=3072,\n",
        "        weights=weights,\n",
        "        progress=progress,\n",
        "        **kwargs,\n",
        "    )"
      ],
      "metadata": {
        "id": "pBGjJNXc8ZdN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = vit_b_16(weights= ViT_B_16_Weights.IMAGENET1K_V1)\n",
        "print(model)"
      ],
      "metadata": {
        "id": "bYjK7-nXAYtP",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "2be7e01111e946529d401302b1b98e71",
            "5b05b1f3aad444ad9d0f51626b0152df",
            "db5ee99d20ee470cbecde88f070da07b",
            "0663381d1ecf49e9963fc95630edd51e",
            "4be6f0ec679e457082787eb7f74a59c3",
            "c96ca1db4f624996a86041f41631e0d1",
            "c5d964514c6a4382a53fbdd2b5053c42",
            "084a8a334dc3487382f51e90e44cea14",
            "b91a4bb0b74b4982bcd995cc34f7262a",
            "c469ad4ab88c40b5ba072208884fcd4e",
            "81d4a8723a0b45e7b87f134367d33c6b"
          ]
        },
        "outputId": "b7db4f14-90bd-44e5-a310-2474d7a4688e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/vit_b_16-c867db91.pth\" to /root/.cache/torch/hub/checkpoints/vit_b_16-c867db91.pth\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0.00/330M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "2be7e01111e946529d401302b1b98e71"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VisionTransformer(\n",
            "  (conv_proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
            "  (encoder): Encoder(\n",
            "    (dropout): Dropout(p=0.0, inplace=False)\n",
            "    (layers): Sequential(\n",
            "      (encoder_layer_0): EncoderBlock(\n",
            "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
            "        (self_attention): MultiheadAttention(\n",
            "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
            "        )\n",
            "        (dropout): Dropout(p=0.0, inplace=False)\n",
            "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
            "        (mlp): MLPBlock(\n",
            "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          (1): GELU(approximate=none)\n",
            "          (2): Dropout(p=0.0, inplace=False)\n",
            "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
            "          (4): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (encoder_layer_1): EncoderBlock(\n",
            "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
            "        (self_attention): MultiheadAttention(\n",
            "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
            "        )\n",
            "        (dropout): Dropout(p=0.0, inplace=False)\n",
            "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
            "        (mlp): MLPBlock(\n",
            "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          (1): GELU(approximate=none)\n",
            "          (2): Dropout(p=0.0, inplace=False)\n",
            "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
            "          (4): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (encoder_layer_2): EncoderBlock(\n",
            "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
            "        (self_attention): MultiheadAttention(\n",
            "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
            "        )\n",
            "        (dropout): Dropout(p=0.0, inplace=False)\n",
            "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
            "        (mlp): MLPBlock(\n",
            "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          (1): GELU(approximate=none)\n",
            "          (2): Dropout(p=0.0, inplace=False)\n",
            "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
            "          (4): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (encoder_layer_3): EncoderBlock(\n",
            "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
            "        (self_attention): MultiheadAttention(\n",
            "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
            "        )\n",
            "        (dropout): Dropout(p=0.0, inplace=False)\n",
            "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
            "        (mlp): MLPBlock(\n",
            "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          (1): GELU(approximate=none)\n",
            "          (2): Dropout(p=0.0, inplace=False)\n",
            "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
            "          (4): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (encoder_layer_4): EncoderBlock(\n",
            "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
            "        (self_attention): MultiheadAttention(\n",
            "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
            "        )\n",
            "        (dropout): Dropout(p=0.0, inplace=False)\n",
            "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
            "        (mlp): MLPBlock(\n",
            "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          (1): GELU(approximate=none)\n",
            "          (2): Dropout(p=0.0, inplace=False)\n",
            "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
            "          (4): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (encoder_layer_5): EncoderBlock(\n",
            "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
            "        (self_attention): MultiheadAttention(\n",
            "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
            "        )\n",
            "        (dropout): Dropout(p=0.0, inplace=False)\n",
            "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
            "        (mlp): MLPBlock(\n",
            "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          (1): GELU(approximate=none)\n",
            "          (2): Dropout(p=0.0, inplace=False)\n",
            "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
            "          (4): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (encoder_layer_6): EncoderBlock(\n",
            "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
            "        (self_attention): MultiheadAttention(\n",
            "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
            "        )\n",
            "        (dropout): Dropout(p=0.0, inplace=False)\n",
            "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
            "        (mlp): MLPBlock(\n",
            "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          (1): GELU(approximate=none)\n",
            "          (2): Dropout(p=0.0, inplace=False)\n",
            "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
            "          (4): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (encoder_layer_7): EncoderBlock(\n",
            "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
            "        (self_attention): MultiheadAttention(\n",
            "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
            "        )\n",
            "        (dropout): Dropout(p=0.0, inplace=False)\n",
            "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
            "        (mlp): MLPBlock(\n",
            "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          (1): GELU(approximate=none)\n",
            "          (2): Dropout(p=0.0, inplace=False)\n",
            "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
            "          (4): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (encoder_layer_8): EncoderBlock(\n",
            "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
            "        (self_attention): MultiheadAttention(\n",
            "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
            "        )\n",
            "        (dropout): Dropout(p=0.0, inplace=False)\n",
            "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
            "        (mlp): MLPBlock(\n",
            "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          (1): GELU(approximate=none)\n",
            "          (2): Dropout(p=0.0, inplace=False)\n",
            "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
            "          (4): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (encoder_layer_9): EncoderBlock(\n",
            "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
            "        (self_attention): MultiheadAttention(\n",
            "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
            "        )\n",
            "        (dropout): Dropout(p=0.0, inplace=False)\n",
            "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
            "        (mlp): MLPBlock(\n",
            "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          (1): GELU(approximate=none)\n",
            "          (2): Dropout(p=0.0, inplace=False)\n",
            "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
            "          (4): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (encoder_layer_10): EncoderBlock(\n",
            "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
            "        (self_attention): MultiheadAttention(\n",
            "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
            "        )\n",
            "        (dropout): Dropout(p=0.0, inplace=False)\n",
            "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
            "        (mlp): MLPBlock(\n",
            "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          (1): GELU(approximate=none)\n",
            "          (2): Dropout(p=0.0, inplace=False)\n",
            "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
            "          (4): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (encoder_layer_11): EncoderBlock(\n",
            "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
            "        (self_attention): MultiheadAttention(\n",
            "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
            "        )\n",
            "        (dropout): Dropout(p=0.0, inplace=False)\n",
            "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
            "        (mlp): MLPBlock(\n",
            "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          (1): GELU(approximate=none)\n",
            "          (2): Dropout(p=0.0, inplace=False)\n",
            "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
            "          (4): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (ln): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
            "    (class_heads): Sequential(\n",
            "      (class_head_0): Sequential(\n",
            "        (0): Linear(in_features=768, out_features=102, bias=True)\n",
            "        (1): Softmax(dim=1)\n",
            "      )\n",
            "      (class_head_1): Sequential(\n",
            "        (0): Linear(in_features=768, out_features=102, bias=True)\n",
            "        (1): Softmax(dim=1)\n",
            "      )\n",
            "      (class_head_2): Sequential(\n",
            "        (0): Linear(in_features=768, out_features=102, bias=True)\n",
            "        (1): Softmax(dim=1)\n",
            "      )\n",
            "      (class_head_3): Sequential(\n",
            "        (0): Linear(in_features=768, out_features=102, bias=True)\n",
            "        (1): Softmax(dim=1)\n",
            "      )\n",
            "      (class_head_4): Sequential(\n",
            "        (0): Linear(in_features=768, out_features=102, bias=True)\n",
            "        (1): Softmax(dim=1)\n",
            "      )\n",
            "      (class_head_5): Sequential(\n",
            "        (0): Linear(in_features=768, out_features=102, bias=True)\n",
            "        (1): Softmax(dim=1)\n",
            "      )\n",
            "      (class_head_6): Sequential(\n",
            "        (0): Linear(in_features=768, out_features=102, bias=True)\n",
            "        (1): Softmax(dim=1)\n",
            "      )\n",
            "      (class_head_7): Sequential(\n",
            "        (0): Linear(in_features=768, out_features=102, bias=True)\n",
            "        (1): Softmax(dim=1)\n",
            "      )\n",
            "      (class_head_8): Sequential(\n",
            "        (0): Linear(in_features=768, out_features=102, bias=True)\n",
            "        (1): Softmax(dim=1)\n",
            "      )\n",
            "      (class_head_9): Sequential(\n",
            "        (0): Linear(in_features=768, out_features=102, bias=True)\n",
            "        (1): Softmax(dim=1)\n",
            "      )\n",
            "      (class_head_10): Sequential(\n",
            "        (0): Linear(in_features=768, out_features=102, bias=True)\n",
            "        (1): Softmax(dim=1)\n",
            "      )\n",
            "      (class_head_11): Sequential(\n",
            "        (0): Linear(in_features=768, out_features=102, bias=True)\n",
            "        (1): Softmax(dim=1)\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (heads): Sequential(\n",
            "    (head): Linear(in_features=768, out_features=1000, bias=True)\n",
            "  )\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "mX1nXPK6r9wD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "weights = ViT_B_16_Weights.IMAGENET1K_V1\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "model = vit_b_16(progress=True, weights=weights).to(device)\n",
        "\n",
        "preprocess = weights.transforms()\n",
        "transform = Compose([ToTensor(), Lambda(lambda x: preprocess(x))])\n",
        "\n",
        "train_dataset = Flowers102(root='.', \n",
        "                        split='test',\n",
        "                        download=True,\n",
        "                        transform=transform)\n",
        "\n",
        "test_dataset = Flowers102(root='.', \n",
        "                       split='train',\n",
        "                       download=True,\n",
        "                       transform=transform)\n",
        "\n",
        "n_train = int(0.8*len(train_dataset))\n",
        "n_valid = len(train_dataset) - n_train\n",
        "\n",
        "train_dataset, valid_dataset = random_split(train_dataset, (n_train, n_valid))\n",
        "print(len(train_dataset), len(valid_dataset), len(test_dataset))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BH-8z-U9pLwg",
        "outputId": "517023bc-29b6-4364-aac7-e984efb73958"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4919 1230 1020\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def with_freezed_params(model):\n",
        "    for param in model.parameters():\n",
        "      param.requires_grad = False\n",
        "    return model\n",
        "\n",
        "\n",
        "class BasicViT(nn.Module):\n",
        "\n",
        "    def __init__(self, *args, **kwargs):\n",
        "      super().__init__()\n",
        "      # vit = vit_b_16(weights =  ViT_B_16_Weights.IMAGENET1K_V1)\n",
        "      # print(len([p for p in vit.parameters() if p.requires_grad]))\n",
        "      self.vit = with_freezed_params( vit_b_16(weights =  ViT_B_16_Weights.IMAGENET1K_V1))\n",
        "      self.vit.heads = nn.Linear(in_features=768, out_features=kwargs.get('out_heads', 102), bias=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.vit(x)"
      ],
      "metadata": {
        "id": "g0QQLM25s8az"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "basic_model = BasicViT()\n",
        "print(basic_model)\n",
        "torch.save(basic_model, \"BASIC_MODEL.pt\")"
      ],
      "metadata": {
        "id": "5E8CCWcytLnP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "931149c9-807e-4699-8c9d-cc8fffcaaf5f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BasicViT(\n",
            "  (vit): VisionTransformer(\n",
            "    (conv_proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
            "    (encoder): Encoder(\n",
            "      (dropout): Dropout(p=0.0, inplace=False)\n",
            "      (layers): Sequential(\n",
            "        (encoder_layer_0): EncoderBlock(\n",
            "          (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
            "          (self_attention): MultiheadAttention(\n",
            "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
            "          )\n",
            "          (dropout): Dropout(p=0.0, inplace=False)\n",
            "          (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
            "          (mlp): MLPBlock(\n",
            "            (0): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            (1): GELU(approximate=none)\n",
            "            (2): Dropout(p=0.0, inplace=False)\n",
            "            (3): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (4): Dropout(p=0.0, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (encoder_layer_1): EncoderBlock(\n",
            "          (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
            "          (self_attention): MultiheadAttention(\n",
            "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
            "          )\n",
            "          (dropout): Dropout(p=0.0, inplace=False)\n",
            "          (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
            "          (mlp): MLPBlock(\n",
            "            (0): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            (1): GELU(approximate=none)\n",
            "            (2): Dropout(p=0.0, inplace=False)\n",
            "            (3): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (4): Dropout(p=0.0, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (encoder_layer_2): EncoderBlock(\n",
            "          (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
            "          (self_attention): MultiheadAttention(\n",
            "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
            "          )\n",
            "          (dropout): Dropout(p=0.0, inplace=False)\n",
            "          (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
            "          (mlp): MLPBlock(\n",
            "            (0): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            (1): GELU(approximate=none)\n",
            "            (2): Dropout(p=0.0, inplace=False)\n",
            "            (3): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (4): Dropout(p=0.0, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (encoder_layer_3): EncoderBlock(\n",
            "          (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
            "          (self_attention): MultiheadAttention(\n",
            "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
            "          )\n",
            "          (dropout): Dropout(p=0.0, inplace=False)\n",
            "          (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
            "          (mlp): MLPBlock(\n",
            "            (0): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            (1): GELU(approximate=none)\n",
            "            (2): Dropout(p=0.0, inplace=False)\n",
            "            (3): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (4): Dropout(p=0.0, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (encoder_layer_4): EncoderBlock(\n",
            "          (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
            "          (self_attention): MultiheadAttention(\n",
            "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
            "          )\n",
            "          (dropout): Dropout(p=0.0, inplace=False)\n",
            "          (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
            "          (mlp): MLPBlock(\n",
            "            (0): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            (1): GELU(approximate=none)\n",
            "            (2): Dropout(p=0.0, inplace=False)\n",
            "            (3): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (4): Dropout(p=0.0, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (encoder_layer_5): EncoderBlock(\n",
            "          (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
            "          (self_attention): MultiheadAttention(\n",
            "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
            "          )\n",
            "          (dropout): Dropout(p=0.0, inplace=False)\n",
            "          (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
            "          (mlp): MLPBlock(\n",
            "            (0): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            (1): GELU(approximate=none)\n",
            "            (2): Dropout(p=0.0, inplace=False)\n",
            "            (3): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (4): Dropout(p=0.0, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (encoder_layer_6): EncoderBlock(\n",
            "          (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
            "          (self_attention): MultiheadAttention(\n",
            "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
            "          )\n",
            "          (dropout): Dropout(p=0.0, inplace=False)\n",
            "          (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
            "          (mlp): MLPBlock(\n",
            "            (0): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            (1): GELU(approximate=none)\n",
            "            (2): Dropout(p=0.0, inplace=False)\n",
            "            (3): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (4): Dropout(p=0.0, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (encoder_layer_7): EncoderBlock(\n",
            "          (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
            "          (self_attention): MultiheadAttention(\n",
            "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
            "          )\n",
            "          (dropout): Dropout(p=0.0, inplace=False)\n",
            "          (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
            "          (mlp): MLPBlock(\n",
            "            (0): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            (1): GELU(approximate=none)\n",
            "            (2): Dropout(p=0.0, inplace=False)\n",
            "            (3): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (4): Dropout(p=0.0, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (encoder_layer_8): EncoderBlock(\n",
            "          (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
            "          (self_attention): MultiheadAttention(\n",
            "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
            "          )\n",
            "          (dropout): Dropout(p=0.0, inplace=False)\n",
            "          (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
            "          (mlp): MLPBlock(\n",
            "            (0): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            (1): GELU(approximate=none)\n",
            "            (2): Dropout(p=0.0, inplace=False)\n",
            "            (3): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (4): Dropout(p=0.0, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (encoder_layer_9): EncoderBlock(\n",
            "          (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
            "          (self_attention): MultiheadAttention(\n",
            "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
            "          )\n",
            "          (dropout): Dropout(p=0.0, inplace=False)\n",
            "          (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
            "          (mlp): MLPBlock(\n",
            "            (0): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            (1): GELU(approximate=none)\n",
            "            (2): Dropout(p=0.0, inplace=False)\n",
            "            (3): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (4): Dropout(p=0.0, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (encoder_layer_10): EncoderBlock(\n",
            "          (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
            "          (self_attention): MultiheadAttention(\n",
            "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
            "          )\n",
            "          (dropout): Dropout(p=0.0, inplace=False)\n",
            "          (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
            "          (mlp): MLPBlock(\n",
            "            (0): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            (1): GELU(approximate=none)\n",
            "            (2): Dropout(p=0.0, inplace=False)\n",
            "            (3): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (4): Dropout(p=0.0, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (encoder_layer_11): EncoderBlock(\n",
            "          (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
            "          (self_attention): MultiheadAttention(\n",
            "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
            "          )\n",
            "          (dropout): Dropout(p=0.0, inplace=False)\n",
            "          (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
            "          (mlp): MLPBlock(\n",
            "            (0): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            (1): GELU(approximate=none)\n",
            "            (2): Dropout(p=0.0, inplace=False)\n",
            "            (3): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (4): Dropout(p=0.0, inplace=False)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (ln): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
            "      (class_heads): Sequential(\n",
            "        (class_head_0): Sequential(\n",
            "          (0): Linear(in_features=768, out_features=102, bias=True)\n",
            "          (1): Softmax(dim=1)\n",
            "        )\n",
            "        (class_head_1): Sequential(\n",
            "          (0): Linear(in_features=768, out_features=102, bias=True)\n",
            "          (1): Softmax(dim=1)\n",
            "        )\n",
            "        (class_head_2): Sequential(\n",
            "          (0): Linear(in_features=768, out_features=102, bias=True)\n",
            "          (1): Softmax(dim=1)\n",
            "        )\n",
            "        (class_head_3): Sequential(\n",
            "          (0): Linear(in_features=768, out_features=102, bias=True)\n",
            "          (1): Softmax(dim=1)\n",
            "        )\n",
            "        (class_head_4): Sequential(\n",
            "          (0): Linear(in_features=768, out_features=102, bias=True)\n",
            "          (1): Softmax(dim=1)\n",
            "        )\n",
            "        (class_head_5): Sequential(\n",
            "          (0): Linear(in_features=768, out_features=102, bias=True)\n",
            "          (1): Softmax(dim=1)\n",
            "        )\n",
            "        (class_head_6): Sequential(\n",
            "          (0): Linear(in_features=768, out_features=102, bias=True)\n",
            "          (1): Softmax(dim=1)\n",
            "        )\n",
            "        (class_head_7): Sequential(\n",
            "          (0): Linear(in_features=768, out_features=102, bias=True)\n",
            "          (1): Softmax(dim=1)\n",
            "        )\n",
            "        (class_head_8): Sequential(\n",
            "          (0): Linear(in_features=768, out_features=102, bias=True)\n",
            "          (1): Softmax(dim=1)\n",
            "        )\n",
            "        (class_head_9): Sequential(\n",
            "          (0): Linear(in_features=768, out_features=102, bias=True)\n",
            "          (1): Softmax(dim=1)\n",
            "        )\n",
            "        (class_head_10): Sequential(\n",
            "          (0): Linear(in_features=768, out_features=102, bias=True)\n",
            "          (1): Softmax(dim=1)\n",
            "        )\n",
            "        (class_head_11): Sequential(\n",
            "          (0): Linear(in_features=768, out_features=102, bias=True)\n",
            "          (1): Softmax(dim=1)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (heads): Linear(in_features=768, out_features=102, bias=True)\n",
            "  )\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "valid_loader = DataLoader(valid_dataset, batch_size=2, shuffle=False)\n",
        "for x,y in valid_loader:\n",
        "  # print(x[0],y[0])\n",
        "  print(x.shape)\n",
        "  break\n",
        "\n",
        "output, early_class = basic_model(x)\n",
        "# print(output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Orq_1a3xpeRm",
        "outputId": "2f2a2259-dd6d-453a-b5b8-5ab68d6eda94"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 3, 224, 224])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(early_class))\n",
        "print(early_class[10].shape)\n",
        "print(early_class[10][0])\n",
        "print(early_class[10][0].sum())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SqKrtpZcdnM6",
        "outputId": "12e59ded-79bc-49a9-a22a-ec507d011ff6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "12\n",
            "torch.Size([2, 102])\n",
            "tensor([0.0078, 0.0114, 0.0118, 0.0114, 0.0130, 0.0077, 0.0114, 0.0098, 0.0085,\n",
            "        0.0108, 0.0154, 0.0088, 0.0109, 0.0100, 0.0105, 0.0109, 0.0091, 0.0082,\n",
            "        0.0099, 0.0090, 0.0092, 0.0096, 0.0085, 0.0091, 0.0100, 0.0074, 0.0091,\n",
            "        0.0158, 0.0097, 0.0099, 0.0103, 0.0099, 0.0082, 0.0090, 0.0097, 0.0104,\n",
            "        0.0110, 0.0069, 0.0091, 0.0113, 0.0070, 0.0082, 0.0092, 0.0120, 0.0098,\n",
            "        0.0104, 0.0086, 0.0067, 0.0082, 0.0101, 0.0088, 0.0108, 0.0094, 0.0114,\n",
            "        0.0091, 0.0096, 0.0077, 0.0085, 0.0078, 0.0133, 0.0121, 0.0090, 0.0137,\n",
            "        0.0094, 0.0112, 0.0098, 0.0109, 0.0085, 0.0084, 0.0111, 0.0075, 0.0072,\n",
            "        0.0075, 0.0085, 0.0124, 0.0110, 0.0106, 0.0083, 0.0121, 0.0095, 0.0126,\n",
            "        0.0099, 0.0093, 0.0105, 0.0128, 0.0118, 0.0069, 0.0076, 0.0094, 0.0096,\n",
            "        0.0073, 0.0089, 0.0103, 0.0091, 0.0099, 0.0072, 0.0129, 0.0098, 0.0104,\n",
            "        0.0105, 0.0108, 0.0065])\n",
            "tensor(1.0000)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def valid(model, loader):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        # initialize the number of correct predictions\n",
        "        correct: int = 0 \n",
        "        N: int = 0\n",
        "\n",
        "        for i, (x, y) in enumerate(loader):\n",
        "            x, y = x.to(device), y.to(device)\n",
        "            N += y.shape[0]\n",
        "\n",
        "            # pass through the network\n",
        "            output, early_class = model(x)\n",
        "\n",
        "            # update the number of correctly predicted examples\n",
        "            correct += sum([torch.argmax(output[k]) == y[k] for k in range(output.shape[0])])\n",
        "\n",
        "    return correct / N  # , early_class\n",
        "\n",
        "\n",
        "def run_epoch(model, optimizer, criterion, loader, optimizer2=None):\n",
        "    \"\"\"param :unfreezed: train the model during the last epoch with unfreezed all wieghts.\"\"\"\n",
        "    model.train()\n",
        "    N: int = 0\n",
        "    \n",
        "    for x, y in loader:\n",
        "        x, y = x.to(device), y.to(device)\n",
        "        N += y.shape[0]\n",
        "\n",
        "        #don't accumulate gradients\n",
        "        optimizer.zero_grad()\n",
        "        if optimizer2:\n",
        "            optimizer2.zero_grad()\n",
        "        output, early_class = model(x)\n",
        "\n",
        "        loss: torch.Tensor = criterion(output, target=y)\n",
        "        #backwards pass through the network\n",
        "        loss.backward()\n",
        "\n",
        "        #apply gradients\n",
        "        optimizer.step()\n",
        "        if optimizer2:\n",
        "            optimizer2.step()\n",
        "\n",
        "    return early_class, y\n",
        "\n",
        "def unfreeze_params(x,y,z):\n",
        "    pass\n",
        "\n",
        "def train_with_params(params, criterion, datasets, unfreezed = False, at_beginning=False):\n",
        "    train_dataset, valid_dataset, test_dataset = datasets[\"train\"], datasets[\"valid\"], datasets[\"test\"]\n",
        "    train_loader = DataLoader(train_dataset, batch_size=params[\"batch_size\"], shuffle=True)\n",
        "    valid_loader = DataLoader(valid_dataset, batch_size=params['batch_size'], shuffle=False)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=params['batch_size'], shuffle=False)\n",
        "\n",
        "    # test_model = BasicViT().to(device)  # for random weight initialization\n",
        "    test_model = torch.load(\"BASIC_MODEL.pt\")\n",
        "    test_model = test_model.to(device)\n",
        "    # test_model = torch.load(\"BASIC_MODEL.pt\").to(device)\n",
        "\n",
        "    optimizer = torch.optim.Adam([p for p in test_model.parameters() if p.requires_grad], lr=params['lr'])\n",
        "    optimizer2 = None\n",
        "\n",
        "    if unfreezed:\n",
        "        unfreeze_params(test_model, unfreeze_params=True, all=False)\n",
        "        print(\"After switching grads ON: \",len([p for p in test_model.parameters() if p.requires_grad]))\n",
        "        optimizer2 = torch.optim.Adam([p for p in test_model.parameters() if p.requires_grad], lr=params['lr'])\n",
        "        unfreeze_params(test_model, unfreeze_params=False, all=False)\n",
        "\n",
        "    for epoch in range(params[\"epochs_num\"]):\n",
        "        if at_beginning and epoch == 0 and unfreezed:\n",
        "            print(\"Training with unfreezed params, first epoch\")\n",
        "            unfreeze_params(test_model, unfreeze_params=True, all=True)\n",
        "            epoch_train_loss = run_epoch(test_model, optimizer, criterion, train_loader, optimizer2=optimizer2)\n",
        "            unfreeze_params(test_model, unfreeze_params=False)\n",
        "\n",
        "        elif not at_beginning and epoch == params[\"epochs_num\"]-1 and unfreezed:\n",
        "            print(\"Training with unfreezed params, last epoch\")\n",
        "            unfreeze_params(test_model, unfreeze_params=True, all=True)\n",
        "            epoch_train_loss = run_epoch(test_model, optimizer, criterion, train_loader, optimizer2=optimizer2)\n",
        "            # unfreeze_params(test_model, unfreeze_params=False)\n",
        "        else:\n",
        "            print(f\"Training with freezed params, epoch = {epoch}\")\n",
        "            early_classif, y = run_epoch(test_model, optimizer, criterion, train_loader)\n",
        "\n",
        "            # let's observe the probabilities of the last picture, how its largest probability acts and changes label\n",
        "            max_elem = 0\n",
        "            max_idx = 0\n",
        "            for idx, elem in enumerate(early_classif[-1][-1]):\n",
        "                if max_elem < elem.cpu().numpy():\n",
        "                    max_elem = elem.cpu().numpy()\n",
        "                    max_idx = idx\n",
        "            print(f\"Last picture - max probability = {max_elem:.5f} with idx = {max_idx}, actual class = {y[-1]}\")\n",
        "\n",
        "    model_valid_acc = valid(test_model, valid_loader)\n",
        "    # model_test_acc = valid(test_model, test_loader)\n",
        "\n",
        "    return model_valid_acc, test_model #, model_test_acc\n",
        "\n",
        "\n",
        "def make_params_grid(param_grid, max_num_sets=None, randomize=True):\n",
        "    to_list = lambda x: [x] if not isinstance(x, Iterable) else x\n",
        "\n",
        "    params = {k: to_list(v) for k, v in param_grid.items()}\n",
        "    if randomize:\n",
        "        grid = shuffle(ParameterGrid(params))\n",
        "        return grid[:max_num_sets]\n",
        "\n",
        "    return ParameterGrid(params)\n",
        "\n",
        "\n",
        "def find_best_params(param_grid, max_num_sets, criterion, datasets, unfreezed=False, at_beginning=False):\n",
        "    best_params = {}\n",
        "    best_valid_acc = 0.0\n",
        "\n",
        "    param_grid = make_params_grid(param_grid, max_num_sets, randomize=True)\n",
        "\n",
        "    for i, params in enumerate(param_grid):\n",
        "        # model_valid_acc, model_test_acc = train_with_params(params, optimizer, criterion, datasets)\n",
        "        model_valid_acc, trained_model = train_with_params(params, criterion, datasets, unfreezed, at_beginning)\n",
        "        print(f'Model: {i} trained, valid accuracy: {model_valid_acc:.4f}')\n",
        "\n",
        "        if model_valid_acc > best_valid_acc:\n",
        "            best_valid_acc = model_valid_acc\n",
        "            best_params = params\n",
        "            torch.save(trained_model, \"BEST_PARAMS_MODEL.pt\")\n",
        "\n",
        "    print(f'Best params: {best_params}, best validation accuracy: {best_valid_acc}')\n",
        "    test_loader = DataLoader(datasets[\"test\"], batch_size=best_params['batch_size'], shuffle=False)\n",
        "    best_model = torch.load(\"BEST_PARAMS_MODEL.pt\")\n",
        "    print(f'Test accuracy: {valid(best_model, test_loader)}' )\n",
        "\n",
        "    return best_params"
      ],
      "metadata": {
        "id": "Ytn_HooAiTLb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "param_grid = {\n",
        "                'lr': [0.001],\n",
        "                'epochs_num': [4],\n",
        "                'batch_size': [32],\n",
        "             }\n",
        "\n",
        "max_num_sets = 1\n",
        "\n",
        "datasets = {\n",
        "            \"train\": train_dataset,\n",
        "            \"valid\": valid_dataset,\n",
        "            \"test\": test_dataset\n",
        "            }\n",
        "\n",
        "best_params = find_best_params(param_grid, max_num_sets, criterion, datasets, unfreezed=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D8CijlaHiWt5",
        "outputId": "0ecd69e7-e3e6-44e3-bf0f-ede0b9f2e3b7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training with freezed params, epoch = 0\n",
            "Last picture - max probability = 0.01650 with idx = 60, actual class = 16\n",
            "Training with freezed params, epoch = 1\n",
            "Last picture - max probability = 0.01675 with idx = 55, actual class = 48\n",
            "Training with freezed params, epoch = 2\n",
            "Last picture - max probability = 0.01567 with idx = 96, actual class = 50\n",
            "Training with freezed params, epoch = 3\n",
            "Last picture - max probability = 0.01628 with idx = 19, actual class = 89\n",
            "Model: 0 trained, valid accuracy: 0.9301\n",
            "Best params: {'lr': 0.001, 'epochs_num': 4, 'batch_size': 32}, best validation accuracy: 0.930081307888031\n",
            "Test accuracy: 0.9117647409439087\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data_transforms = {\n",
        "    'train': transforms.Compose([\n",
        "        transforms.Resize((464,464)),\n",
        "        transforms.RandomRotation(15,),\n",
        "        transforms.RandomCrop(448),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.507, 0.487, 0.441], std=[0.267, 0.256, 0.276])\n",
        "    ]),\n",
        "'val': transforms.Compose([\n",
        "        transforms.Resize((448,448)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.507, 0.487, 0.441], std=[0.267, 0.256, 0.276])\n",
        "    ]),"
      ],
      "metadata": {
        "id": "wFEb6HRV_6WB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "dodać augmentacje do zbioru, "
      ],
      "metadata": {
        "id": "-HgjZvQfBdL3"
      }
    }
  ]
}