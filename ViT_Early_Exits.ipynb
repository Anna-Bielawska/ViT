{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchvision.models import ViT_B_16_Weights\n",
        "from torchvision.datasets import Flowers102\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "from torchvision.transforms import ToTensor, Compose, Lambda\n",
        "from src.early_exits import vit_b_16\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "metadata": {
        "id": "BhICkRZF5vCP"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "weights = ViT_B_16_Weights.IMAGENET1K_V1\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "preprocess = weights.transforms()\n",
        "transform = Compose([ToTensor(), Lambda(lambda x: preprocess(x))])\n",
        "\n",
        "# dataset loading - test set switched with training set - done on purpose\n",
        "train_dataset = Flowers102(root='.', \n",
        "                        split='test',\n",
        "                        download=True,\n",
        "                        transform=transform)\n",
        "\n",
        "test_dataset = Flowers102(root='.', \n",
        "                       split='train',\n",
        "                       download=True,\n",
        "                       transform=transform)\n",
        "\n",
        "n_train = int(0.8*len(train_dataset))\n",
        "n_valid = len(train_dataset) - n_train\n",
        "\n",
        "train_dataset, valid_dataset = random_split(train_dataset, (n_train, n_valid))\n",
        "print(len(train_dataset), len(valid_dataset), len(test_dataset))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BH-8z-U9pLwg",
        "outputId": "5406423d-3941-4302-f2aa-806a5ba5d572"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4919 1230 1020\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def freeze_params(model):\n",
        "    for idx, param in enumerate(model.parameters()):\n",
        "        if idx >= 150:\n",
        "            param.requires_grad = True\n",
        "        else:\n",
        "            param.requires_grad = False\n",
        "    return model\n",
        "\n",
        "basic_model = freeze_params(vit_b_16(weights=ViT_B_16_Weights.IMAGENET1K_V1))\n",
        "torch.save(basic_model, \"BASIC_MODEL.pt\")\n",
        "print(basic_model)"
      ],
      "metadata": {
        "id": "g0QQLM25s8az",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eed0dd9c-90f7-42a7-9d10-c5c2010478c6"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CustomViT(\n",
            "  (conv_proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
            "  (encoder): CustomEncoder(\n",
            "    (dropout): Dropout(p=0.0, inplace=False)\n",
            "    (layers): Sequential(\n",
            "      (encoder_layer_0): EncoderBlock(\n",
            "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
            "        (self_attention): MultiheadAttention(\n",
            "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
            "        )\n",
            "        (dropout): Dropout(p=0.0, inplace=False)\n",
            "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
            "        (mlp): MLPBlock(\n",
            "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          (1): GELU(approximate='none')\n",
            "          (2): Dropout(p=0.0, inplace=False)\n",
            "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
            "          (4): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (encoder_layer_1): EncoderBlock(\n",
            "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
            "        (self_attention): MultiheadAttention(\n",
            "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
            "        )\n",
            "        (dropout): Dropout(p=0.0, inplace=False)\n",
            "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
            "        (mlp): MLPBlock(\n",
            "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          (1): GELU(approximate='none')\n",
            "          (2): Dropout(p=0.0, inplace=False)\n",
            "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
            "          (4): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (encoder_layer_2): EncoderBlock(\n",
            "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
            "        (self_attention): MultiheadAttention(\n",
            "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
            "        )\n",
            "        (dropout): Dropout(p=0.0, inplace=False)\n",
            "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
            "        (mlp): MLPBlock(\n",
            "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          (1): GELU(approximate='none')\n",
            "          (2): Dropout(p=0.0, inplace=False)\n",
            "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
            "          (4): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (encoder_layer_3): EncoderBlock(\n",
            "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
            "        (self_attention): MultiheadAttention(\n",
            "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
            "        )\n",
            "        (dropout): Dropout(p=0.0, inplace=False)\n",
            "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
            "        (mlp): MLPBlock(\n",
            "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          (1): GELU(approximate='none')\n",
            "          (2): Dropout(p=0.0, inplace=False)\n",
            "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
            "          (4): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (encoder_layer_4): EncoderBlock(\n",
            "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
            "        (self_attention): MultiheadAttention(\n",
            "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
            "        )\n",
            "        (dropout): Dropout(p=0.0, inplace=False)\n",
            "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
            "        (mlp): MLPBlock(\n",
            "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          (1): GELU(approximate='none')\n",
            "          (2): Dropout(p=0.0, inplace=False)\n",
            "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
            "          (4): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (encoder_layer_5): EncoderBlock(\n",
            "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
            "        (self_attention): MultiheadAttention(\n",
            "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
            "        )\n",
            "        (dropout): Dropout(p=0.0, inplace=False)\n",
            "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
            "        (mlp): MLPBlock(\n",
            "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          (1): GELU(approximate='none')\n",
            "          (2): Dropout(p=0.0, inplace=False)\n",
            "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
            "          (4): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (encoder_layer_6): EncoderBlock(\n",
            "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
            "        (self_attention): MultiheadAttention(\n",
            "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
            "        )\n",
            "        (dropout): Dropout(p=0.0, inplace=False)\n",
            "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
            "        (mlp): MLPBlock(\n",
            "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          (1): GELU(approximate='none')\n",
            "          (2): Dropout(p=0.0, inplace=False)\n",
            "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
            "          (4): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (encoder_layer_7): EncoderBlock(\n",
            "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
            "        (self_attention): MultiheadAttention(\n",
            "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
            "        )\n",
            "        (dropout): Dropout(p=0.0, inplace=False)\n",
            "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
            "        (mlp): MLPBlock(\n",
            "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          (1): GELU(approximate='none')\n",
            "          (2): Dropout(p=0.0, inplace=False)\n",
            "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
            "          (4): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (encoder_layer_8): EncoderBlock(\n",
            "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
            "        (self_attention): MultiheadAttention(\n",
            "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
            "        )\n",
            "        (dropout): Dropout(p=0.0, inplace=False)\n",
            "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
            "        (mlp): MLPBlock(\n",
            "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          (1): GELU(approximate='none')\n",
            "          (2): Dropout(p=0.0, inplace=False)\n",
            "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
            "          (4): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (encoder_layer_9): EncoderBlock(\n",
            "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
            "        (self_attention): MultiheadAttention(\n",
            "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
            "        )\n",
            "        (dropout): Dropout(p=0.0, inplace=False)\n",
            "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
            "        (mlp): MLPBlock(\n",
            "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          (1): GELU(approximate='none')\n",
            "          (2): Dropout(p=0.0, inplace=False)\n",
            "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
            "          (4): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (encoder_layer_10): EncoderBlock(\n",
            "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
            "        (self_attention): MultiheadAttention(\n",
            "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
            "        )\n",
            "        (dropout): Dropout(p=0.0, inplace=False)\n",
            "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
            "        (mlp): MLPBlock(\n",
            "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          (1): GELU(approximate='none')\n",
            "          (2): Dropout(p=0.0, inplace=False)\n",
            "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
            "          (4): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (encoder_layer_11): EncoderBlock(\n",
            "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
            "        (self_attention): MultiheadAttention(\n",
            "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
            "        )\n",
            "        (dropout): Dropout(p=0.0, inplace=False)\n",
            "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
            "        (mlp): MLPBlock(\n",
            "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          (1): GELU(approximate='none')\n",
            "          (2): Dropout(p=0.0, inplace=False)\n",
            "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
            "          (4): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (ln): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
            "    (early_exit_heads): ModuleList(\n",
            "      (0): Sequential(\n",
            "        (0): Linear(in_features=768, out_features=102, bias=True)\n",
            "      )\n",
            "      (1): Sequential(\n",
            "        (0): Linear(in_features=768, out_features=102, bias=True)\n",
            "      )\n",
            "      (2): Sequential(\n",
            "        (0): Linear(in_features=768, out_features=102, bias=True)\n",
            "      )\n",
            "      (3): Sequential(\n",
            "        (0): Linear(in_features=768, out_features=102, bias=True)\n",
            "      )\n",
            "      (4): Sequential(\n",
            "        (0): Linear(in_features=768, out_features=102, bias=True)\n",
            "      )\n",
            "      (5): Sequential(\n",
            "        (0): Linear(in_features=768, out_features=102, bias=True)\n",
            "      )\n",
            "      (6): Sequential(\n",
            "        (0): Linear(in_features=768, out_features=102, bias=True)\n",
            "      )\n",
            "      (7): Sequential(\n",
            "        (0): Linear(in_features=768, out_features=102, bias=True)\n",
            "      )\n",
            "      (8): Sequential(\n",
            "        (0): Linear(in_features=768, out_features=102, bias=True)\n",
            "      )\n",
            "      (9): Sequential(\n",
            "        (0): Linear(in_features=768, out_features=102, bias=True)\n",
            "      )\n",
            "      (10): Sequential(\n",
            "        (0): Linear(in_features=768, out_features=102, bias=True)\n",
            "      )\n",
            "      (11): Sequential(\n",
            "        (0): Linear(in_features=768, out_features=102, bias=True)\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (heads): Sequential(\n",
            "    (head): Linear(in_features=768, out_features=102, bias=True)\n",
            "  )\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def valid(model, loader) -> float:\n",
        "    model.eval() # switch dropouts off\n",
        "    with torch.no_grad():\n",
        "        # initialize the number of correct predictions\n",
        "        correct: int = 0 \n",
        "        N: int = 0\n",
        "\n",
        "        for i, (x, y) in enumerate(loader):\n",
        "            x, y = x.to(device), y.to(device)\n",
        "            N += y.shape[0]\n",
        "\n",
        "            # pass through the network, remeber about early exits outputs\n",
        "            output, early_class = model(x)\n",
        "\n",
        "            # update the number of correctly predicted examples\n",
        "            correct += sum([torch.argmax(output[k]) == y[k] for k in range(output.shape[0])])\n",
        "\n",
        "    return correct / N\n",
        "\n",
        "\n",
        "def run_epoch(model, optimizer, criterion, loader, optimizer2=None):\n",
        "    model.train()  # switch on dropouts\n",
        "    N: int = 0\n",
        "    \n",
        "    for x, y in loader:\n",
        "        x, y = x.to(device), y.to(device)\n",
        "        N += y.shape[0]\n",
        "\n",
        "        #don't accumulate gradients\n",
        "        optimizer.zero_grad()\n",
        "        if optimizer2:\n",
        "            optimizer2.zero_grad()\n",
        "\n",
        "        output, early_class = model(x)\n",
        "\n",
        "        loss: torch.Tensor = criterion(output, target=y)\n",
        "        #backwards pass through the network\n",
        "        loss.backward()\n",
        "\n",
        "        #apply gradients\n",
        "        optimizer.step()\n",
        "        if optimizer2:\n",
        "            optimizer2.step()\n",
        "\n",
        "    return early_class, y\n",
        "\n",
        "\n",
        "def train_with_params(params, criterion, datasets):\n",
        "    train_dataset, valid_dataset = datasets[\"train\"], datasets[\"valid\"]\n",
        "    train_loader = DataLoader(train_dataset, batch_size=params[\"batch_size\"], shuffle=True)\n",
        "    valid_loader = DataLoader(valid_dataset, batch_size=params['batch_size'], shuffle=False)\n",
        "\n",
        "    test_model = torch.load(\"BASIC_MODEL.pt\")\n",
        "    test_model = test_model.to(device)\n",
        "\n",
        "    weights = [p for p in test_model.parameters() if p.requires_grad]\n",
        "    optimizer = torch.optim.Adam(weights, lr=params['lr'])\n",
        "\n",
        "    for epoch in range(params[\"epochs_num\"]):\n",
        "        print(f\"Training with freezed params, epoch = {epoch}\")\n",
        "        early_classif, y = run_epoch(test_model, optimizer, criterion, train_loader)\n",
        "        # print(len(early_classif))  # 12 x [m, 102]\n",
        "        # for each classification head, get probabilities for the last batch containing m <= 32 pictures\n",
        "\n",
        "        # let's observe the probabilities of the last picture,\n",
        "        # how its largest probability acts and predicts labels\n",
        "        max_elem = 0\n",
        "        max_idx = 0\n",
        "        for idx, class_head in enumerate(early_classif):\n",
        "            for cls, elem in enumerate(class_head[-1]):\n",
        "                if max_elem < elem.detach().cpu().numpy():\n",
        "                    max_elem = elem.detach().cpu().numpy()\n",
        "                    max_idx = cls\n",
        "            print(f\"Early exit nr {idx}, last picture - max probability = {max_elem:.5f} for class = {max_idx}, actual class = {y[-1]}\")\n",
        "            max_elem = 0\n",
        "            max_idx = 0\n",
        "\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "    model_valid_acc = valid(test_model, valid_loader)\n",
        "    model_valid_acc = model_valid_acc.detach().cpu().numpy()\n",
        "\n",
        "    return model_valid_acc, test_model"
      ],
      "metadata": {
        "id": "Ytn_HooAiTLb"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "params = {\n",
        "          'lr': 0.001,\n",
        "          'epochs_num': 4,\n",
        "          'batch_size': 32,\n",
        "          }\n",
        "\n",
        "datasets = {\n",
        "            \"train\": train_dataset,\n",
        "            \"valid\": valid_dataset,\n",
        "            \"test\": test_dataset\n",
        "            }\n",
        "\n",
        "acc, trained_model = train_with_params(params=params, criterion=criterion, datasets=datasets)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D8CijlaHiWt5",
        "outputId": "02d53dc6-a4e6-47c0-8487-48445516b715"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training with freezed params, epoch = 0\n",
            "Early exit nr 0, last picture - max probability = 0.01430 for class = 66, actual class = 46\n",
            "Early exit nr 1, last picture - max probability = 0.01246 for class = 27, actual class = 46\n",
            "Early exit nr 2, last picture - max probability = 0.01444 for class = 100, actual class = 46\n",
            "Early exit nr 3, last picture - max probability = 0.01451 for class = 61, actual class = 46\n",
            "Early exit nr 4, last picture - max probability = 0.01687 for class = 70, actual class = 46\n",
            "Early exit nr 5, last picture - max probability = 0.01573 for class = 88, actual class = 46\n",
            "Early exit nr 6, last picture - max probability = 0.01495 for class = 77, actual class = 46\n",
            "Early exit nr 7, last picture - max probability = 0.01803 for class = 92, actual class = 46\n",
            "Early exit nr 8, last picture - max probability = 0.01344 for class = 60, actual class = 46\n",
            "Early exit nr 9, last picture - max probability = 0.01406 for class = 33, actual class = 46\n",
            "Early exit nr 10, last picture - max probability = 0.01289 for class = 59, actual class = 46\n",
            "Early exit nr 11, last picture - max probability = 0.01397 for class = 79, actual class = 46\n",
            "Training with freezed params, epoch = 1\n",
            "Early exit nr 0, last picture - max probability = 0.01400 for class = 66, actual class = 74\n",
            "Early exit nr 1, last picture - max probability = 0.01330 for class = 27, actual class = 74\n",
            "Early exit nr 2, last picture - max probability = 0.01496 for class = 100, actual class = 74\n",
            "Early exit nr 3, last picture - max probability = 0.01463 for class = 61, actual class = 74\n",
            "Early exit nr 4, last picture - max probability = 0.01686 for class = 70, actual class = 74\n",
            "Early exit nr 5, last picture - max probability = 0.01437 for class = 88, actual class = 74\n",
            "Early exit nr 6, last picture - max probability = 0.01525 for class = 57, actual class = 74\n",
            "Early exit nr 7, last picture - max probability = 0.01822 for class = 92, actual class = 74\n",
            "Early exit nr 8, last picture - max probability = 0.01398 for class = 3, actual class = 74\n",
            "Early exit nr 9, last picture - max probability = 0.01407 for class = 58, actual class = 74\n",
            "Early exit nr 10, last picture - max probability = 0.01269 for class = 45, actual class = 74\n",
            "Early exit nr 11, last picture - max probability = 0.01566 for class = 7, actual class = 74\n",
            "Training with freezed params, epoch = 2\n",
            "Early exit nr 0, last picture - max probability = 0.01434 for class = 66, actual class = 50\n",
            "Early exit nr 1, last picture - max probability = 0.01284 for class = 27, actual class = 50\n",
            "Early exit nr 2, last picture - max probability = 0.01448 for class = 100, actual class = 50\n",
            "Early exit nr 3, last picture - max probability = 0.01447 for class = 61, actual class = 50\n",
            "Early exit nr 4, last picture - max probability = 0.01645 for class = 70, actual class = 50\n",
            "Early exit nr 5, last picture - max probability = 0.01451 for class = 84, actual class = 50\n",
            "Early exit nr 6, last picture - max probability = 0.01631 for class = 57, actual class = 50\n",
            "Early exit nr 7, last picture - max probability = 0.01760 for class = 92, actual class = 50\n",
            "Early exit nr 8, last picture - max probability = 0.01517 for class = 3, actual class = 50\n",
            "Early exit nr 9, last picture - max probability = 0.01466 for class = 19, actual class = 50\n",
            "Early exit nr 10, last picture - max probability = 0.01393 for class = 18, actual class = 50\n",
            "Early exit nr 11, last picture - max probability = 0.01467 for class = 10, actual class = 50\n",
            "Training with freezed params, epoch = 3\n",
            "Early exit nr 0, last picture - max probability = 0.01331 for class = 66, actual class = 55\n",
            "Early exit nr 1, last picture - max probability = 0.01280 for class = 21, actual class = 55\n",
            "Early exit nr 2, last picture - max probability = 0.01458 for class = 100, actual class = 55\n",
            "Early exit nr 3, last picture - max probability = 0.01339 for class = 61, actual class = 55\n",
            "Early exit nr 4, last picture - max probability = 0.01805 for class = 70, actual class = 55\n",
            "Early exit nr 5, last picture - max probability = 0.01423 for class = 84, actual class = 55\n",
            "Early exit nr 6, last picture - max probability = 0.01443 for class = 57, actual class = 55\n",
            "Early exit nr 7, last picture - max probability = 0.01608 for class = 92, actual class = 55\n",
            "Early exit nr 8, last picture - max probability = 0.01326 for class = 54, actual class = 55\n",
            "Early exit nr 9, last picture - max probability = 0.01332 for class = 50, actual class = 55\n",
            "Early exit nr 10, last picture - max probability = 0.01360 for class = 58, actual class = 55\n",
            "Early exit nr 11, last picture - max probability = 0.01629 for class = 26, actual class = 55\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Accuracy on the validation dataset:\", acc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GBsrQHx6lQGz",
        "outputId": "5cc8db5e-0733-4789-b7f8-e3fd11d77610"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy on the validation dataset: 0.94715446\n"
          ]
        }
      ]
    }
  ]
}